{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-02-03T21:08:43.118273Z",
     "iopub.status.busy": "2025-02-03T21:08:43.117876Z",
     "iopub.status.idle": "2025-02-03T21:08:43.122050Z",
     "shell.execute_reply": "2025-02-03T21:08:43.121022Z",
     "shell.execute_reply.started": "2025-02-03T21:08:43.118249Z"
    }
   },
   "outputs": [],
   "source": [
    "# # This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# # It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# # For example, here's several helpful packages to load\n",
    "\n",
    "# import numpy as np # linear algebra\n",
    "# import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# # Input data files are available in the read-only \"../input/\" directory\n",
    "# # For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "# import os\n",
    "# for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "#     for filename in filenames:\n",
    "#         print(os.path.join(dirname, filename))\n",
    "\n",
    "# # You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# # You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install InstaGeo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'InstaGeo-E2E-Geospatial-ML'...\n",
      "remote: Enumerating objects: 374, done.\u001b[K\n",
      "remote: Counting objects: 100% (212/212), done.\u001b[K\n",
      "remote: Compressing objects: 100% (142/142), done.\u001b[K\n",
      "remote: Total 374 (delta 127), reused 116 (delta 67), pack-reused 162 (from 1)\u001b[K\n",
      "Receiving objects: 100% (374/374), 1.43 MiB | 3.29 MiB/s, done.\n",
      "Resolving deltas: 100% (194/194), done.\n"
     ]
    }
   ],
   "source": [
    "# Clone the InstaGeo-E2E-Geospatial-ML repository from GitHub\n",
    "repository_url = \"https://github.com/instadeepai/InstaGeo-E2E-Geospatial-ML\"\n",
    "!git clone {repository_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already up to date.\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd InstaGeo-E2E-Geospatial-ML\n",
    "git pull"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-03T21:08:43.560270Z",
     "iopub.status.busy": "2025-02-03T21:08:43.560042Z",
     "iopub.status.idle": "2025-02-03T21:08:59.865705Z",
     "shell.execute_reply": "2025-02-03T21:08:59.864913Z",
     "shell.execute_reply.started": "2025-02-03T21:08:43.560251Z"
    }
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%%bash\n",
    "# Navigate to the cloned InstaGeo-E2E-Geospatial-ML directory\n",
    "cd /kaggle/working/InstaGeo-E2E-Geospatial-ML\n",
    "# Stash any local changes to avoid conflicts when switching branches\n",
    "git stash\n",
    "#Switch to the 'geo-ai-hack' branch, which likely contains specific code for the Geo AI Hackathon\n",
    "git checkout geo-ai-hack\n",
    "# Install the InstaGeo package \n",
    "pip install -e .[all]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from pyproj import CRS, Transformer\n",
    "import rasterio\n",
    "os.environ[\"HYDRA_FULL_ERROR\"] =\"1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rasterio.transform import from_origin\n",
    "from tqdm import tqdm\n",
    "import torch \n",
    "\n",
    "def compute_ndvi(red_band, nir_band):\n",
    "    \"\"\"Compute NDVI using PyTorch tensors.\"\"\"\n",
    "    ndvi = (nir_band - red_band) / (nir_band + red_band + 1e-6)  # Avoid division by zero\n",
    "    return torch.clamp(ndvi, min=-1.0, max=1.0)\n",
    "\n",
    "\n",
    "\n",
    "def compute_mean_std_from_csv(csv_path):\n",
    "    \"\"\"\n",
    "    Compute mean and standard deviation for the 7 spectral bands across all temporal dimensions (total 21 bands),\n",
    "    using .tif file paths provided in the CSV file under the 'Input' column.\n",
    "\n",
    "    Args:\n",
    "        csv_path (str): Path to CSV file containing the list of .tif files.\n",
    "\n",
    "    Returns:\n",
    "        tuple: Mean and standard deviation for the 7 bands across 3 temporal slices.\n",
    "    \"\"\"\n",
    "    sum_pixels = None\n",
    "    sum_squared_pixels = None\n",
    "    total_pixels = 0\n",
    "\n",
    "    # Read CSV file\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # Ensure the CSV has an \"Input\" column\n",
    "    if \"Input\" not in df.columns:\n",
    "        raise ValueError(\"CSV file must contain a column named 'Input' with .tif file paths.\")\n",
    "\n",
    "    file_paths = df[\"Input\"].tolist()  # Extract file paths\n",
    "\n",
    "    for file_path in tqdm(file_paths):\n",
    "        if not os.path.exists(file_path):\n",
    "            print(f\"Skipping {file_path}: File not found.\")\n",
    "            continue\n",
    "\n",
    "        with rasterio.open(file_path) as src:\n",
    "            img = src.read()  # Shape: (21, H, W)\n",
    "            img_tensor = torch.tensor(img, dtype=torch.float32)\n",
    "\n",
    "            if img_tensor.shape[0] != 21:\n",
    "                print(f\"Skipping {file_path}: Expected 21 bands, found {img_tensor.shape[0]}\")\n",
    "                continue\n",
    "\n",
    "            # Extract only the 7 bands from each temporal sequence\n",
    "            selected_bands = torch.cat([img_tensor[:7], img_tensor[7:14], img_tensor[14:21]])\n",
    "\n",
    "            if sum_pixels is None:\n",
    "                sum_pixels = torch.zeros(7, dtype=torch.float32)\n",
    "                sum_squared_pixels = torch.zeros(7, dtype=torch.float32)\n",
    "\n",
    "            sum_pixels += selected_bands[:7].mean(dim=(1, 2))  # Compute mean across spatial dimensions\n",
    "            sum_squared_pixels += (selected_bands[:7] ** 2).mean(dim=(1, 2))  # Compute squared mean\n",
    "            total_pixels += 1\n",
    "\n",
    "    if total_pixels == 0:\n",
    "        raise ValueError(\"No valid files found in the provided CSV. Check file paths.\")\n",
    "\n",
    "    mean = sum_pixels / total_pixels\n",
    "    std = torch.sqrt(sum_squared_pixels / total_pixels - mean ** 2)\n",
    "\n",
    "    print(\"Computed Mean:\", mean.tolist())\n",
    "    print(\"Computed Std:\", std.tolist())\n",
    "    return mean.tolist(), std.tolist()\n",
    "\n",
    "def add_ndvi_in_chips(input_dir, output_dir):\n",
    "    \"\"\"\n",
    "    Process Sentinel-Landsat harmonized chips, compute NDVI, and save 21-band .tif files.\n",
    "    \n",
    "    Args:\n",
    "        input_dir (str): Path to input directory containing .tif chips.\n",
    "        output_dir (str): Path to save processed .tif files.\n",
    "    \"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    for file in tqdm(os.listdir(input_dir)):\n",
    "        if not file.endswith(\".tif\"):\n",
    "            continue\n",
    "        \n",
    "        input_path = os.path.join(input_dir, file)\n",
    "        output_path = os.path.join(output_dir, file)\n",
    "        \n",
    "        with rasterio.open(input_path) as src:\n",
    "            img = src.read()  # Shape: (18, H, W)\n",
    "            meta = src.meta.copy()\n",
    "            \n",
    "            # Ensure we have exactly 18 bands\n",
    "            if img.shape[0] != 18:\n",
    "                print(f\"Skipping {file}: Expected 18 bands, found {img.shape[0]}\")\n",
    "                continue\n",
    "            \n",
    "            H, W = img.shape[1], img.shape[2]\n",
    "            img_tensor = torch.tensor(img, dtype=torch.float32)\n",
    "            \n",
    "            # Compute NDVI for each temporal slice (Bands 3=Red, Bands 4=NIR)\n",
    "            ndvi_1 = compute_ndvi(img_tensor[3], img_tensor[4])\n",
    "            ndvi_2 = compute_ndvi(img_tensor[9], img_tensor[10])\n",
    "            ndvi_3 = compute_ndvi(img_tensor[15], img_tensor[16])\n",
    "            \n",
    "            # Ensure NDVI values are stored as float32\n",
    "            ndvi_1 = ndvi_1.to(dtype=torch.float32)\n",
    "            ndvi_2 = ndvi_2.to(dtype=torch.float32)\n",
    "            ndvi_3 = ndvi_3.to(dtype=torch.float32)\n",
    "            \n",
    "            # Create new tensor with 21 bands\n",
    "            img_new = torch.empty((21, H, W), dtype=torch.float32)\n",
    "            img_new[:7] = torch.cat([img_tensor[:6], ndvi_1.unsqueeze(0)])  # First temporal\n",
    "            img_new[7:14] = torch.cat([img_tensor[6:12], ndvi_2.unsqueeze(0)])  # Second temporal\n",
    "            img_new[14:21] = torch.cat([img_tensor[12:18], ndvi_3.unsqueeze(0)])  # Third temporal\n",
    "            \n",
    "            # Convert back to numpy for saving\n",
    "            img_new_np = img_new.numpy()\n",
    "            meta.update({\"count\": 21, \"dtype\": 'float32'})\n",
    "            \n",
    "            with rasterio.open(output_path, \"w\", **meta) as dst:\n",
    "                dst.write(img_new_np.astype(np.float32))\n",
    "\n",
    "\n",
    "\n",
    "def filter_nodata_labels(input_csv, output_csv, remove_nan_observation=False, nodata_value=-9999):\n",
    "    \"\"\"\n",
    "    Filters out rows where the corresponding label raster contains only -1 values.\n",
    "    Optionally replaces all -1 values with a specified NoData value.\n",
    "\n",
    "    Args:\n",
    "        input_csv (str): Path to the input CSV file.\n",
    "        output_csv (str): Path to save the filtered CSV file.\n",
    "        remove_nan_observation (bool): If True, replaces -1 values in the label raster with NoData.\n",
    "        nodata_value (int, optional): Value to use as NoData when replacing -1. Default is -9999.\n",
    "\n",
    "    Returns:\n",
    "        None (Saves the filtered CSV and prints stats).\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(input_csv)\n",
    "\n",
    "    # Ensure the CSV has \"Input\" and \"Label\" columns\n",
    "    if \"Input\" not in df.columns or \"Label\" not in df.columns:\n",
    "        raise ValueError(\"CSV file must contain 'Input' and 'Label' columns with file paths.\")\n",
    "\n",
    "    original_count = len(df)\n",
    "    nan_observations = 0 \n",
    "    valid_rows = []\n",
    "\n",
    "    for _, row in tqdm(df.iterrows(), total=original_count):\n",
    "        label_path = row[\"Label\"]\n",
    "        \n",
    "        if not os.path.exists(label_path):\n",
    "            print(f\"Skipping {label_path}: File not found.\")\n",
    "            continue\n",
    "        \n",
    "        with rasterio.open(label_path) as src:\n",
    "            label_data = src.read(1)  # Assuming label is a single-band raster\n",
    "            meta = src.meta.copy()\n",
    "\n",
    "            unique_values = set(label_data.flatten())\n",
    "\n",
    "            # Retain only if label contains at least one 0 or 1\n",
    "            if 0 in unique_values or 1 in unique_values:\n",
    "                valid_rows.append(row)\n",
    "\n",
    "                # If remove_nan_observation is enabled, replace -1 with NoData\n",
    "                if remove_nan_observation:\n",
    "                    label_data[label_data == -1] = nodata_value\n",
    "                    meta.update({\"nodata\": nodata_value})\n",
    "\n",
    "                    # Save the modified label raster\n",
    "                    with rasterio.open(label_path, \"w\", **meta) as dst:\n",
    "                        dst.write(label_data, 1)\n",
    "                    nan_observations+=1\n",
    "                    \n",
    "    filtered_count = len(valid_rows)\n",
    "    removed_count = original_count - filtered_count\n",
    "\n",
    "    # Save new CSV with valid rows\n",
    "    new_df = pd.DataFrame(valid_rows)\n",
    "    new_df.to_csv(output_csv, index=False)\n",
    "\n",
    "    # Print statistics\n",
    "    print(f\"Total rows in original CSV: {original_count}\")\n",
    "    print(f\"Rows removed (only contained -1 values): {removed_count}\")\n",
    "    print(f\"Rows remaining after filtering: {filtered_count}\")\n",
    "    if remove_nan_observation:\n",
    "        print(f\"Nan observation removed: {nan_observations}\")\n",
    "\n",
    "    return filtered_count, removed_count\n",
    "\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 10429/10429 [02:52<00:00, 60.55it/s]\n"
     ]
    }
   ],
   "source": [
    "add_ndvi_in_chips(\"hls_train/hls_train/chips\",\"hls_ndvi_train/chips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████| 2405/2405 [01:02<00:00, 38.76it/s]\n"
     ]
    }
   ],
   "source": [
    "add_ndvi_in_chips(\"hls_test/hls_test/chips\",\"hls_ndvi_test/chips\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_label_mapping(root_dir, input_subdir, output_csv):\n",
    "    \"\"\"\n",
    "    Generate a CSV mapping input chips to corresponding segmentation maps.\n",
    "\n",
    "    Args:\n",
    "        root_dir (str or Path): Root directory containing the subdirectories for chips and segmentation maps.\n",
    "        input_subdir (str): Subdirectory path for chips within the root directory.\n",
    "        output_csv (str or Path): Output path for the generated CSV file.\n",
    "    \"\"\"\n",
    "    root_dir = Path(root_dir)\n",
    "    chips_orig = os.listdir(root_dir / input_subdir / \"chips\")\n",
    "    if os.path.exists(root_dir / input_subdir / \"seg_maps\"):\n",
    "        add_label = True\n",
    "    else:\n",
    "        add_label = False\n",
    "\n",
    "    chips = [chip.replace(\"chip\", f\"{input_subdir}/chips/chip\") for chip in chips_orig]\n",
    "\n",
    "    if add_label:\n",
    "        seg_maps = [chip.replace(\"chip\", f\"{input_subdir}/seg_maps/seg_map\") for chip in chips_orig]\n",
    "        df = pd.DataFrame({\"Input\": chips, \"Label\": seg_maps})\n",
    "    else:\n",
    "        df = pd.DataFrame({\"Input\": chips})\n",
    "    df.to_csv(output_csv, index=False)\n",
    "    \n",
    "    print(f\"Number of rows is: {df.shape[0]}\")\n",
    "    print(f\"CSV generated and saved to: {output_csv}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/krschap/foss/geoaihack2024/geo-ai-hack\n"
     ]
    }
   ],
   "source": [
    "# set data folder path\n",
    "import os \n",
    "\n",
    "current_path= os.getcwd() \n",
    "\n",
    "print(current_path)\n",
    "# input_dir= os.path.join(current_path,\"/kaggle/input/geo-ai-hack\")\n",
    "input_dir = current_path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows is: 10429\n",
      "CSV generated and saved to: train_ndvi_ds.csv\n",
      "Number of rows is: 2404\n",
      "CSV generated and saved to: test_ndvi_ds.csv\n"
     ]
    }
   ],
   "source": [
    "# Generate label mappings for the training and testing datasets\n",
    "generate_label_mapping(input_dir, 'hls_ndvi_train', \"train_ndvi_ds.csv\")\n",
    "generate_label_mapping(input_dir, 'hls_ndvi_test', \"test_ndvi_ds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|████████████████████████████████▌                         | 5862/10429 [00:17<00:12, 358.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping hls_ndvi_train/seg_maps/seg_map_20200501_S30_T37NCC_2020094T073611_5_3.tif.aux.xml: File not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████| 10429/10429 [00:30<00:00, 343.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total rows in original CSV: 10429\n",
      "Rows removed (only contained -1 values): 1\n",
      "Rows remaining after filtering: 10428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(10428, 1)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filter_nodata_labels(\"train_ndvi_ds.csv\",\"train_ndvi_ds_filter.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_validation_data(mapping_csv, validation_split=0.3):\n",
    "    \"\"\"\n",
    "    Split data into training and validation sets based on a CSV file mapping `chips` and `seg_maps`.\n",
    "\n",
    "    Args:\n",
    "        mapping_csv (str or Path): Path to the CSV file containing the mapping between `chips` and `seg_maps`.\n",
    "        data_dir (str or Path): Path to the merged directory containing all files.\n",
    "        validation_dir (str or Path): Path to the new directory for validation files.\n",
    "        validation_split (float): Fraction of the data to use as the validation set.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(mapping_csv)\n",
    "    df = df.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "\n",
    "    num_val = int(len(df) * validation_split)\n",
    "    train_df = df[num_val:]\n",
    "    val_df = df[:num_val]\n",
    "    train_df.to_csv(\"train_ndvi_split.csv\",index=False)    \n",
    "    print(f\"CSV train split  saved to: train_ndvi_split.csv\")\n",
    "    val_df.to_csv(\"validation_ndvi_split.csv\",index=False)    \n",
    "    print(f\"CSV validation split  saved to: validation_split.csv\")\n",
    "    \n",
    "    return \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV train split  saved to: train_split.csv\n",
      "CSV validation split  saved to: validation_split.csv\n"
     ]
    }
   ],
   "source": [
    "# Split the training dataset into training and validation sets\n",
    "split_validation_data(\n",
    "    mapping_csv=\"train_ndvi_ds.csv\",\n",
    "    validation_split=0.3\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## InstaGeo - Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating our training and validation splits, we can move on to fine-tuning a model that includes a Prithvi backbone paired with a classification head. For regression tasks, the classification head can easily be replaced with a suitable regression head. Additionally, if a completely different model architecture is needed, it can be designed and implemented within this framework."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_yml(filepath):\n",
    "    \"\"\"Load data from a YAML file.\n",
    "\n",
    "    Args:\n",
    "        filepath (str | Path): The path to the YAML file.\n",
    "\n",
    "    Returns:\n",
    "        Dict: The loaded data, or None if the file does not exist.\n",
    "    \"\"\"\n",
    "    filepath=Path(filepath)\n",
    "    with filepath.open() as f:\n",
    "        return yaml.safe_load(f)\n",
    "        \n",
    "def save_yml(data,filepath):\n",
    "    \"\"\"Save data to a YAML file.\n",
    "\n",
    "    Args:\n",
    "        data (Dict): The data to save.\n",
    "        filepath (str | Path): The file path to save the YAML to.\n",
    "    \"\"\"\n",
    "    filepath = Path(filepath)\n",
    "    with filepath.open(\"w\") as f:\n",
    "        yaml.dump(data, f)\n",
    "    print(f\"Data saved to {filepath}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Launch Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "First, compute the mean and standard deviation for the dataset using the InstaGeo command. Then update the corresponding configuration file [locust.yaml](https://github.com/instadeepai/InstaGeo-E2E-Geospatial-ML/blob/main/instageo/model/configs/locust.yaml). In this case, it has already been done for you. However, if you change the dataset split or modify the training data, you should run the command again to compute the new mean and standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_mean_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "nings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "Seed set to 1042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-04 23:34:12,035][__main__][INFO] - Script: /home/krschap/foss/geoaihack2024/geo-ai-hack/InstaGeo-E2E-Geospatial-ML/instageo/model/run.py\n",
      "[2025-02-04 23:34:12,037][__main__][INFO] - Imported hydra config:\n",
      "checkpoint_path: null\n",
      "dataloader:\n",
      "  bands:\n",
      "  - 0\n",
      "  - 1\n",
      "  - 2\n",
      "  - 3\n",
      "  - 4\n",
      "  - 5\n",
      "  - 6\n",
      "  - 7\n",
      "  - 8\n",
      "  - 9\n",
      "  - 10\n",
      "  - 11\n",
      "  - 12\n",
      "  - 13\n",
      "- 14\n",
      "  - 15\n",
      "  - 16\n",
      "  - 17\n",
      "  - 18\n",
      "  - 19\n",
      "  - 20\n",
      "  constant_multiplier: 1.0\n",
      "  img_size: 256\n",
      "  mean:\n",
      "- 623.2724609375\n",
      "  - 1247.657958984375\n",
      "  - 1772.24169921875\n",
      "  - 2371.256103515625\n",
      "625 2862.867431640\n",
      "  - 2357.759765625\n",
      "  no_data_value: -9999\n",
      "  reduce_to_zero: false\n",
      "  replace_label:\n",
      "  - -9999\n",
      " -1\n",
      "  std:\n",
      "  - 2182.050048828125\n",
      "  - 2248.420654296875\n",
      "  - 2302.53515625\n",
      "  - 2372.204345703125\n",
      "398.52685546875\n",
      "  - 2292.96435546875\n",
      "  temporal_dim: 3\n",
      "mean:\n",
      "- 670.5441284179688\n",
      "5 1267.797485351562\n",
      "- 1772.599365234375\n",
      "- 2415.69091796875\n",
      "- 2879.2431640625\n",
      "- 2337.822509765625\n",
      "mode: stats\n",
      "model:\n",
      "freeze_backbone: false\n",
      "  num_classes: 2\n",
      "output_dir: null\n",
      "root_dir: .\n",
      "std:\n",
      "- 2146.305419921875\n",
      ".416259765625\n",
      "- 2247.03515625\n",
      "- 2310.74755859375\n",
      "- 2322.708984375\n",
      "- 2211.968505859375\n",
      "test:\n",
      "ize: 256\n",
      "  img_size: 256\n",
      "  mask_cloud: false\n",
      "  stride: 256\n",
      "test_filepath: null\n",
      "train:\n",
      "8 batch_size: \n",
      "  class_weights:\n",
      "  - 1\n",
      "  - 1\n",
      "  ignore_index: -1\n",
      "  learning_rate: 0.0001\n",
      "  num_epochs: 5\n",
      "ecay: 0.05\n",
      "train_filepath: train_ndvi_ds.csv\n",
      "valid_filepath: null\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error executing job with overrides: ['root_dir=.', 'train.batch_size=8', 'train.num_epochs=5', 'mode=stats', 'train_filepath=train_ndvi_ds.csv']\n",
      "Traceback (most recent call last):\n",
      "y>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "rschap/foss/geoaihack2024/geo-ai-hack/InstaGeo-E2E-Geospatial-ML/instageo/model/run.py\", line 749, in <module>\n",
      "    main()\n",
      " line 94, in decorated_maingeoaihack2024/env/lib/python3.12/site-packages/hydra/main.py\",\n",
      "    _run_hydra(\n",
      ".12/site-packages/hydra/_internal/utils.py\", line 394, in _run_hydra\n",
      "    _run_app(\n",
      "chap/foss/geoaihack2024/env/lib/python3.12/site-packages/hydra/_internal/utils.py\", line 457, in _run_app\n",
      "    run_and_report(\n",
      "hydra/_internal/utils.py\", line 223, in run_and_reportn3.12/site-packages/\n",
      "    raise ex\n",
      "hack2024/env/lib/python3.12/site-packages/hydra/_internal/utils.py\", line 220, in run_and_report\n",
      " return func()\n",
      "           ^^^^^^\n",
      "ckages/hydra/_internal/utils.py\", line 458, in <lambda>3.12/site-pa\n",
      "    lambda: hydra.run(\n",
      "^           ^^^^^^^^^\n",
      "\", line 132, in runap/foss/geoaihack2024/env/lib/python3.12/site-packages/hydra/_internal/hydra.py\n",
      "    _ = ret.return_value\n",
      "        ^^^^^^^^^^^^^^^^\n",
      "aihack2024/env/lib/python3.12/site-packages/hydra/core/utils.py\", line 260, in return_value\n",
      "e self._return_value\n",
      "/core/utils.py\", line 186, in run_job024/env/lib/python3.12/site-packages/hydra\n",
      "    ret.return_value = task_function(task_cfg)\n",
      "        ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ospatial-ML/instageo/model/run.py\", line 519, in mainInstaGeo-E2E-Ge\n",
      "    mean, std = compute_mean_std(train_loader)\n",
      "                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "/InstaGeo-E2E-Geospatial-ML/instageo/model/run.py\", line 450, in compute_mean_std\n",
      " data_loader: _ in\n",
      "data/dataloader.py\", line 701, in __next__nv/lib/python3.12/site-packages/torch/utils/\n",
      "    data = self._next_data()\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "er.py\", line 1465, in _next_datahack2024/env/lib/python3.12/site-packages/torch/utils/data/dataload\n",
      "    return self._process_data(data)\n",
      "^^^^       ^^^^^^^^^^^^^^^^^^^^\n",
      "loader.py\", line 1491, in _process_data4/env/lib/python3.12/site-packages/torch/utils/data/data\n",
      "    data.reraise()\n",
      "env/lib/python3.12/site-packages/torch/_utils.py\", line 715, in reraise\n",
      "    raise exception\n",
      "rror: Caught RuntimeError in DataLoader worker process 0.\n",
      ":riginal Traceback (most recent call last)\n",
      "worker.py\", line 351, in _worker_loop024/env/lib/python3.12/site-packages/torch/utils/data/_utils/\n",
      "ined]ata = fetcher.fetch(index)  # type: ignore[possibly-undef\n",
      "           ^^^^^^^^^^^^^^^^^^^^\n",
      "te-packages/torch/utils/data/_utils/fetch.py\", line 52, in fetch\n",
      "dx in possibly_batched_index] for i\n",
      "            ~~~~~~~~~~~~^^^^^\n",
      "/geo-ai-hack/InstaGeo-E2E-Geospatial-ML/instageo/model/dataloader.py\", line 426, in __getitem__\n",
      "return self.preprocess_func(arr_x, arr_y)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "e/krschap/foss/geoaihack2024/geo-ai-hack/InstaGeo-E2E-Geospatial-ML/instageo/model/dataloader.py\", line 166, in process_and_augment\n",
      "td, temporal_size)ormalize_and_convert_to_tensor(ims, label, mean, s\n",
      "^^^^^            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "/dataloader.py\", line 127, in normalize_and_convert_to_tensor-E2E-Geospatial-ML/instageo/model\n",
      " for im in ims_tensor]).permute(rm(im)\n",
      "                              ^^^^^^^^\n",
      "eoaihack2024/env/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "le \"/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "^^^^^      ^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "ms/transforms.py\", line 277, in forward4/env/lib/python3.12/site-packages/torchvision/transfor\n",
      "lace)eturn F.normalize(tensor, self.mean, self.std, self.inp\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "eoaihack2024/env/lib/python3.12/site-packages/torchvision/transforms/functional.py\", line 350, in normalize\n",
      "    return F_t.normalize(tensor, mean=mean, std=std, inplace=inplace)\n",
      "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "hon3.12/site-packages/torchvision/transforms/_functional_tensor.py\", line 928, in normalize\n",
      "rn tensor.sub_(mean).div_(std)\n",
      "           ^^^^^^^^^^^^^^^^^\n",
      "ust match the size of tensor b (6) at non-singleton dimension 0\n",
      "\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command 'b'python -m instageo.model.run --config-name=locust-ndvi \\\\\\n    root_dir=\".\" \\\\\\n    train.batch_size=8 \\\\\\n    train.num_epochs=5 \\\\\\n    mode=stats \\\\\\n    train_filepath=\"train_ndvi_ds.csv\" \\\\\\n'' returned non-zero exit status 1.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mget_ipython\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_cell_magic\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbash\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpython -m instageo.model.run --config-name=locust-ndvi \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    root_dir=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    train.batch_size=8 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    train.num_epochs=5 \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    mode=stats \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m    train_filepath=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain_ndvi_ds.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;130;43;01m\\n\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/foss/geoaihack2024/env/lib/python3.12/site-packages/IPython/core/interactiveshell.py:2543\u001b[0m, in \u001b[0;36mInteractiveShell.run_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2541\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuiltin_trap:\n\u001b[1;32m   2542\u001b[0m     args \u001b[38;5;241m=\u001b[39m (magic_arg_s, cell)\n\u001b[0;32m-> 2543\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2545\u001b[0m \u001b[38;5;66;03m# The code below prevents the output from being displayed\u001b[39;00m\n\u001b[1;32m   2546\u001b[0m \u001b[38;5;66;03m# when using magics with decorator @output_can_be_silenced\u001b[39;00m\n\u001b[1;32m   2547\u001b[0m \u001b[38;5;66;03m# when the last Python token in the expression is a ';'.\u001b[39;00m\n\u001b[1;32m   2548\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(fn, magic\u001b[38;5;241m.\u001b[39mMAGIC_OUTPUT_CAN_BE_SILENCED, \u001b[38;5;28;01mFalse\u001b[39;00m):\n",
      "File \u001b[0;32m~/foss/geoaihack2024/env/lib/python3.12/site-packages/IPython/core/magics/script.py:159\u001b[0m, in \u001b[0;36mScriptMagics._make_script_magic.<locals>.named_script_magic\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m    157\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    158\u001b[0m     line \u001b[38;5;241m=\u001b[39m script\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshebang\u001b[49m\u001b[43m(\u001b[49m\u001b[43mline\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcell\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/foss/geoaihack2024/env/lib/python3.12/site-packages/IPython/core/magics/script.py:336\u001b[0m, in \u001b[0;36mScriptMagics.shebang\u001b[0;34m(self, line, cell)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mraise_error \u001b[38;5;129;01mand\u001b[39;00m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    332\u001b[0m     \u001b[38;5;66;03m# If we get here and p.returncode is still None, we must have\u001b[39;00m\n\u001b[1;32m    333\u001b[0m     \u001b[38;5;66;03m# killed it but not yet seen its return code. We don't wait for it,\u001b[39;00m\n\u001b[1;32m    334\u001b[0m     \u001b[38;5;66;03m# in case it's stuck in uninterruptible sleep. -9 = SIGKILL\u001b[39;00m\n\u001b[1;32m    335\u001b[0m     rc \u001b[38;5;241m=\u001b[39m p\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m9\u001b[39m\n\u001b[0;32m--> 336\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(rc, cell)\n",
      "\u001b[0;31mCalledProcessError\u001b[0m: Command 'b'python -m instageo.model.run --config-name=locust-ndvi \\\\\\n    root_dir=\".\" \\\\\\n    train.batch_size=8 \\\\\\n    train.num_epochs=5 \\\\\\n    mode=stats \\\\\\n    train_filepath=\"train_ndvi_ds.csv\" \\\\\\n'' returned non-zero exit status 1."
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python -m instageo.model.run --config-name=locust-ndvi \\\n",
    "    root_dir=\".\" \\\n",
    "    train.batch_size=8 \\\n",
    "    train.num_epochs=5 \\\n",
    "    mode=stats \\\n",
    "    train_filepath=\"train_ndvi_ds.csv\" \\"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 56%|████████████████████████████████▉                          | 5831/10429 [01:41<01:19, 58.11it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping hls_ndvi_train/chips/chip_20200501_S30_T37NCC_2020094T073611_5_3.tif.aux.xml: File not found.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 10429/10429 [03:01<00:00, 57.53it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computed Mean: [695.5580444335938, 1293.4241943359375, 1792.435546875, 2443.89111328125, 2880.999267578125, 2321.597900390625, 0.062481388449668884]\n",
      "Computed Std: [3357.875244140625, 3497.693359375, 3630.5771484375, 3794.192138671875, 3892.0703125, 3700.383544921875, 0.11115093529224396]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([695.5580444335938,\n",
       "  1293.4241943359375,\n",
       "  1792.435546875,\n",
       "  2443.89111328125,\n",
       "  2880.999267578125,\n",
       "  2321.597900390625,\n",
       "  0.062481388449668884],\n",
       " [3357.875244140625,\n",
       "  3497.693359375,\n",
       "  3630.5771484375,\n",
       "  3794.192138671875,\n",
       "  3892.0703125,\n",
       "  3700.383544921875,\n",
       "  0.11115093529224396])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_mean_std_from_csv(\"train_ndvi_ds.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to InstaGeo-E2E-Geospatial-ML/instageo/model/configs/locust-ndvi.yaml.\n"
     ]
    }
   ],
   "source": [
    "# Updat locust file\n",
    "# Load the Locust model configuration file\n",
    "locust_cfg_path=\"InstaGeo-E2E-Geospatial-ML/instageo/model/configs/locust-ndvi.yaml\"\n",
    "# Load the YAML configuration into a dictionary\n",
    "locust_cfg=load_yml(locust_cfg_path)\n",
    "# Update the mean and standard deviation values in the configuration\n",
    "locust_cfg[\"mean\"]=[670.5441284179688, 1267.7974853515625, 1772.599365234375, 2415.69091796875, 2879.2431640625, 2337.822509765625,0.062481388449668884]\n",
    "locust_cfg[\"std\"]=[2146.305419921875, 2203.416259765625, 2247.03515625, 2310.74755859375, 2322.708984375, 2211.968505859375,0.11115093529224396]\n",
    "# Save the updated configuration back to the YAML file\n",
    "save_yml(locust_cfg,locust_cfg_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/krschap/foss/geoaihack2024/geo-ai-hack\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "cwd = os.getcwd() \n",
    "print(cwd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Is CUDA supported by this system? True\n",
      "CUDA version: 12.4\n",
      "ID of current CUDA device:0\n",
      "Name of current CUDA device:NVIDIA GeForce RTX 4090 Laptop GPU\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "# Storing ID of current CUDA device\n",
    "cuda_id = torch.cuda.current_device()\n",
    "print(f\"ID of current CUDA device:{torch.cuda.current_device()}\")\n",
    "\t\n",
    "print(f\"Name of current CUDA device:{torch.cuda.get_device_name(cuda_id)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/timm/models/layers/__init__.py:48: FutureWarning: Importing from timm.models.layers is deprecated, please import via timm.layers\n",
      "  warnings.warn(f\"Importing from {__name__} is deprecated, please import via timm.layers\", FutureWarning)\n",
      "Seed set to 1042\n",
      "[2025-02-05 08:28:10,197][__main__][INFO] - Script: /home/krschap/foss/geoaihack2024/geo-ai-hack/InstaGeo-E2E-Geospatial-ML/instageo/model/run.py\n",
      "[2025-02-05 08:28:10,198][__main__][INFO] - Imported hydra config:\n",
      "checkpoint_path: null\n",
      "dataloader:\n",
      "  bands:\n",
      "  - 0\n",
      "  - 1\n",
      "  - 2\n",
      "  - 3\n",
      "  - 4\n",
      "  - 5\n",
      "  - 6\n",
      "  - 7\n",
      "  - 8\n",
      "  - 9\n",
      "  - 10\n",
      "  - 11\n",
      "  - 12\n",
      "  - 13\n",
      "  - 14\n",
      "  - 15\n",
      "  - 16\n",
      "  - 17\n",
      "  - 18\n",
      "  - 19\n",
      "  - 20\n",
      "  constant_multiplier: 1.0\n",
      "  img_size: 256\n",
      "  mean:\n",
      "  - 623.2724609375\n",
      "  - 1247.657958984375\n",
      "  - 1772.24169921875\n",
      "  - 2371.256103515625\n",
      "  - 2862.867431640625\n",
      "  - 2357.759765625\n",
      "  - 0.062481388449668884\n",
      "  no_data_value: -9999\n",
      "  reduce_to_zero: false\n",
      "  replace_label:\n",
      "  - -9999\n",
      "  - -1\n",
      "  std:\n",
      "  - 2182.050048828125\n",
      "  - 2248.420654296875\n",
      "  - 2302.53515625\n",
      "  - 2372.204345703125\n",
      "  - 2398.52685546875\n",
      "  - 2292.96435546875\n",
      "  - 0.11115093529224396\n",
      "  temporal_dim: 3\n",
      "mean:\n",
      "- 670.5441284179688\n",
      "- 1267.7974853515625\n",
      "- 1772.599365234375\n",
      "- 2415.69091796875\n",
      "- 2879.2431640625\n",
      "- 2337.822509765625\n",
      "- 0.062481388449668884\n",
      "mode: train\n",
      "model:\n",
      "  freeze_backbone: false\n",
      "  num_classes: 2\n",
      "output_dir: null\n",
      "root_dir: .\n",
      "std:\n",
      "- 2146.305419921875\n",
      "- 2203.416259765625\n",
      "- 2247.03515625\n",
      "- 2310.74755859375\n",
      "- 2322.708984375\n",
      "- 2211.968505859375\n",
      "- 0.11115093529224396\n",
      "test:\n",
      "  crop_size: 256\n",
      "  img_size: 256\n",
      "  mask_cloud: false\n",
      "  stride: 256\n",
      "test_filepath: null\n",
      "train:\n",
      "  batch_size: 10\n",
      "  class_weights:\n",
      "  - 1\n",
      "  - 1\n",
      "  ignore_index: -1\n",
      "  learning_rate: 0.0001\n",
      "  num_epochs: 20\n",
      "  weight_decay: 0.15\n",
      "train_filepath: train_ndvi_split.csv\n",
      "valid_filepath: validation_ndvi_split.csv\n",
      "\n",
      "[2025-02-05 08:28:18,911][absl][INFO] - File '/home/krschap/.instageo/prithvi/Prithvi_EO_V1_100M.pt' already exists. Skipping download.\n",
      "[2025-02-05 08:28:18,911][absl][INFO] - File '/home/krschap/.instageo/prithvi/config.yaml' already exists. Skipping download.\n",
      "/home/krschap/foss/geoaihack2024/geo-ai-hack/InstaGeo-E2E-Geospatial-ML/instageo/model/model.py:155: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(weights_path, map_location=\"cpu\")\n",
      "[2025-02-05 08:28:19,632][root][INFO] - GPU is available. Using GPU...\n",
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:654: Checkpoint directory /home/krschap/foss/geoaihack2024/geo-ai-hack/ndvi exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | net       | PrithviSeg       | 134 M  | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "133 M     Trainable params\n",
      "590 K     Non-trainable params\n",
      "134 M     Total params\n",
      "537.703   Total estimated model params size (MB)\n",
      "282       Modules in train mode\n",
      "0         Modules in eval mode\n",
      "Sanity Checking: |                                        | 0/? [00:00<?, ?it/s]/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n",
      "Error executing job with overrides: ['root_dir=.', 'train.batch_size=10', 'train.num_epochs=20', 'mode=train', 'train_filepath=train_ndvi_split.csv', 'valid_filepath=validation_ndvi_split.csv']\n",
      "Traceback (most recent call last):\n",
      "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/home/krschap/foss/geoaihack2024/geo-ai-hack/InstaGeo-E2E-Geospatial-ML/instageo/model/run.py\", line 749, in <module>\n",
      "    main()\n",
      "  File \"/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/hydra/main.py\", line 94, in decorated_main\n",
      "    _run_hydra(\n",
      "  File \"/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/hydra/_internal/utils.py\", line 394, in _run_hydra\n",
      "    _run_app(\n",
      "  File \"/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/hydra/_internal/utils.py\", line 457, in _run_app\n",
      "    run_and_report(\n",
      "  File \"/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/hydra/_internal/utils.py\", line 223, in run_and_report\n",
      "    raise ex\n",
      "  File \"/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/hydra/_internal/utils.py\", line 220, in run_and_report\n",
      "    return func()\n",
      "           ^^^^^^\n",
      "  File \"/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/hydra/_internal/utils.py\", line 458, in <lambda>\n",
      "    lambda: hydra.run(\n",
      "            ^^^^^^^^^^\n",
      "  File \"/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/hydra/_internal/hydra.py\", line 132, in run\n",
      "    _ = ret.return_value\n",
      "        ^^^^^^^^^^^^^^^^\n",
      "  File \"/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/hydra/core/utils.py\", line 260, in return_value\n",
      "    raise self._return_value\n",
      "  File \"/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/hydra/core/utils.py\", line 186, in run_job\n",
      "    ret.return_value = task_function(task_cfg)\n",
      "                       ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/krschap/foss/geoaihack2024/geo-ai-hack/InstaGeo-E2E-Geospatial-ML/instageo/model/run.py\", line 595, in main\n",
      "    trainer.fit(model, train_loader, valid_loader)\n",
      "  File \"/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 539, in fit\n",
      "    call._call_and_handle_interrupt(\n",
      "  File \"/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 47, in _call_and_handle_interrupt\n",
      "    return trainer_fn(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 575, in _fit_impl\n",
      "    self._run(model, ckpt_path=ckpt_path)\n",
      "  File \"/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 982, in _run\n",
      "    results = self._run_stage()\n",
      "              ^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1024, in _run_stage\n",
      "    self._run_sanity_check()\n",
      "  File \"/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/pytorch_lightning/trainer/trainer.py\", line 1053, in _run_sanity_check\n",
      "    val_loop.run()\n",
      "  File \"/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/pytorch_lightning/loops/utilities.py\", line 179, in _decorator\n",
      "    return loop_run(self, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 144, in run\n",
      "    self._evaluation_step(batch, batch_idx, dataloader_idx, dataloader_iter)\n",
      "  File \"/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/pytorch_lightning/loops/evaluation_loop.py\", line 433, in _evaluation_step\n",
      "    output = call._call_strategy_hook(trainer, hook_name, *step_args)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/pytorch_lightning/trainer/call.py\", line 323, in _call_strategy_hook\n",
      "    output = fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/pytorch_lightning/strategies/strategy.py\", line 412, in validation_step\n",
      "    return self.lightning_module.validation_step(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/krschap/foss/geoaihack2024/geo-ai-hack/InstaGeo-E2E-Geospatial-ML/instageo/model/run.py\", line 234, in validation_step\n",
      "    outputs = self.forward(inputs)\n",
      "              ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/krschap/foss/geoaihack2024/geo-ai-hack/InstaGeo-E2E-Geospatial-ML/instageo/model/run.py\", line 205, in forward\n",
      "    return self.net(x)\n",
      "           ^^^^^^^^^^^\n",
      "  File \"/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/krschap/foss/geoaihack2024/geo-ai-hack/InstaGeo-E2E-Geospatial-ML/instageo/model/model.py\", line 230, in forward\n",
      "    features = self.prithvi_100M_backbone(img)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/krschap/foss/geoaihack2024/geo-ai-hack/InstaGeo-E2E-Geospatial-ML/instageo/model/Prithvi.py\", line 309, in forward\n",
      "    x = self.patch_embed(x)\n",
      "        ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/krschap/foss/geoaihack2024/geo-ai-hack/InstaGeo-E2E-Geospatial-ML/instageo/model/Prithvi.py\", line 171, in forward\n",
      "    x = self.proj(x)\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1736, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/torch/nn/modules/module.py\", line 1747, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 725, in forward\n",
      "    return self._conv_forward(input, self.weight, self.bias)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/torch/nn/modules/conv.py\", line 720, in _conv_forward\n",
      "    return F.conv3d(\n",
      "           ^^^^^^^^^\n",
      "RuntimeError: Given groups=1, weight of size [768, 6, 1, 16, 16], expected input[10, 7, 3, 256, 256] to have 6 channels, but got 7 channels instead\n",
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Train the InstaGeo model using the Locust configuration\n",
    "!python -m instageo.model.run  --config-name=locust-ndvi \\\n",
    "    hydra.run.dir=ndvi \\\n",
    "    root_dir=\".\" \\\n",
    "    train.batch_size=10 \\\n",
    "    train.num_epochs=20 \\\n",
    "    mode=train \\\n",
    "    train_filepath=\"train_ndvi_split.csv\" \\\n",
    "    valid_filepath=\"validation_ndvi_split.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model Evaluation\n",
    " To evaluate the model, adjust the `checkpoint_path` argument to point to the desired model checkpoint. The checkpoint file is typically located in the `hydra.run.dir` directory and is named `instageo_best_checkpoint.ckpt`.\n",
    "For example:\n",
    "```\n",
    "/kaggle/working/outputs/first_run/instageo_best_checkpoint.ckpt \n",
    "```\n",
    "Make sure to provide the correct path to the checkpoint file based on your training output directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-04 18:48:40,713][__main__][INFO] - Script: /home/krschap/foss/geoaihack2024/geo-ai-hack/InstaGeo-E2E-Geospatial-ML/instageo/model/run.py\n",
      "[2025-02-04 18:48:40,714][__main__][INFO] - Imported hydra config:\n",
      "t_checkpoint-v1.ckpt/instageo_bes\n",
      "dataloader:\n",
      "  bands:\n",
      "  - 0\n",
      "  - 1\n",
      "  - 2\n",
      "  - 3\n",
      "  - 4\n",
      "  - 5\n",
      "  - 6\n",
      "  - 7\n",
      "  - 8\n",
      "9 - \n",
      "  - 10\n",
      "  - 11\n",
      "  - 12\n",
      "  - 13\n",
      "  - 14\n",
      "  - 15\n",
      "  - 16\n",
      "  - 17\n",
      "  constant_multiplier: 1.0\n",
      "  img_size: 256\n",
      "  mean:\n",
      "  - 623.2724609375\n",
      "  - 1247.657958984375\n",
      "  - 1772.24169921875\n",
      "  - 2371.256103515625\n",
      "2.867431640625\n",
      "  - 2357.759765625\n",
      "  no_data_value: -9999\n",
      "  reduce_to_zero: false\n",
      "  replace_label:\n",
      "- -9999\n",
      "  - -1\n",
      "  std:\n",
      "  - 2182.050048828125\n",
      "  - 2248.420654296875\n",
      "  - 2302.53515625\n",
      "03125372.2043457\n",
      "  - 2398.52685546875\n",
      "  - 2292.96435546875\n",
      "  temporal_dim: 3\n",
      "mean:\n",
      "- 670.5441284179688\n",
      "974853515625\n",
      "- 1772.599365234375\n",
      "- 2415.69091796875\n",
      "- 2879.2431640625\n",
      "- 2337.822509765625\n",
      "mode: eval\n",
      "model:\n",
      "  freeze_backbone: false\n",
      "  num_classes: 2\n",
      "output_dir: null\n",
      "root_dir: .\n",
      "std:\n",
      "875146.305419921\n",
      "- 2203.416259765625\n",
      "- 2247.03515625\n",
      "- 2310.74755859375\n",
      "- 2322.708984375\n",
      "- 2211.968505859375\n",
      ":est\n",
      "  crop_size: 256\n",
      "  img_size: 256\n",
      "  mask_cloud: false\n",
      "  stride: 256\n",
      ".csv_filepath: validation_split\n",
      "train:\n",
      "  batch_size: 16\n",
      "  class_weights:\n",
      "  - 1\n",
      "  - 1\n",
      "  ignore_index: -1\n",
      "  learning_rate: 0.0001\n",
      "  num_epochs: 20\n",
      "  weight_decay: 0.1\n",
      "train_filepath: null\n",
      "valid_filepath: null\n",
      "\n",
      "[2025-02-04 18:48:44,156][absl][INFO] - File '/home/krschap/.instageo/prithvi/Prithvi_EO_V1_100M.pt' already exists. Skipping download.\n",
      "tageo/prithvi/config.yaml' already exists. Skipping download.ins\n",
      "[2025-02-04 18:48:44,976][root][INFO] - GPU is available. Using GPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "You are using a CUDA device ('NVIDIA GeForce RTX 4090 Laptop GPU') that has Tensor Cores. To properly utilize them, you should set `torch.set_float32_matmul_precision('medium' | 'high')` which will trade-off precision for performance. For more details, read https://pytorch.org/docs/stable/generated/torch.set_float32_matmul_precision.html#torch.set_float32_matmul_precision\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "/home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:425: The 'test_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=31` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing DataLoader 0: 100%|██████████| 196/196 [01:03<00:00,  3.08it/s]\n",
      "━━━━━━━━━━━━━━━━━━━━━━━┓━━━━┳━━━━\n",
      " metric       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      DataLoader 0       \u001b[0m\u001b[1m \u001b[0m┃\n",
      "━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━���━━━━━━━━━━━━━━━━━┩\n",
      "\u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8265830278396606    \u001b[0m\u001b[35m \u001b[0m│\n",
      "t_Acc_1_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6637557744979858    \u001b[0m\u001b[35m \u001b[0m│\n",
      "m \u001b[0m\u001b[36m    test_IoU_0_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6095579862594604    \u001b[0m\u001b[35m \u001b[0m│\n",
      "0289001    \u001b[0m\u001b[35m \u001b[0m│oU_1_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.572863996\n",
      "[35m   0.6977682709693909    \u001b[0m\u001b[35m \u001b[0m│m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b\n",
      "\u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7771565318107605    \u001b[0m\u001b[35m \u001b[0m│\n",
      "epoch   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.8265830278396606    \u001b[0m\u001b[35m \u001b[0m│\n",
      "m   test_Recall_1_epoch   \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.6637557744979858    \u001b[0m\u001b[35m \u001b[0m│\n",
      " \u001b[0m\u001b[35m \u001b[0m│     test_aAcc_epoch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.7456339597702026   \n",
      "112751722335815    \u001b[0m\u001b[35m \u001b[0m│ch     \u001b[0m\u001b[36m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m   0.5\n",
      "5m \u001b[0m\u001b[35m   0.5912108421325684    \u001b[0m\u001b[35m \u001b[0m│[0m│\u001b[3\n",
      "─────────────┴───────────────────���───────┘\n",
      "[2025-02-04 18:49:49,533][__main__][INFO] - Evaluation results:\n",
      "815, 'test_aAcc_epoch': 0.7456339597702026, 'test_mIoU_epoch': 0.5912108421325684, 'test_IoU_0_epoch': 0.6095579862594604, 'test_IoU_1_epoch': 0.5728639960289001, 'test_Acc_0_epoch': 0.8265830278396606, 'test_Acc_1_epoch': 0.6637557744979858, 'test_Precision_0_epoch': 0.6977682709693909, 'test_Precision_1_epoch': 0.7771565318107605, 'test_Recall_0_epoch': 0.8265830278396606, 'test_Recall_1_epoch': 0.6637557744979858}]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python -m instageo.model.run --config-name=locust \\\n",
    "    root_dir=\".\" \\\n",
    "    test_filepath=\"validation_split.csv\" \\\n",
    "    train.batch_size=16 \\\n",
    "    checkpoint_path='cwd/instageo_best_checkpoint-v1.ckpt' \\\n",
    "    mode=eval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first run inference on test chips to get the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 1042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-04 18:53:25,152][__main__][INFO] - Script: /home/krschap/foss/geoaihack2024/geo-ai-hack/InstaGeo-E2E-Geospatial-ML/instageo/model/run.py\n",
      "[2025-02-04 18:53:25,154][__main__][INFO] - Imported hydra config:\n",
      "t_checkpoint-v1.ckpt/instageo_bes\n",
      "dataloader:\n",
      "  bands:\n",
      "  - 0\n",
      "  - 1\n",
      "  - 2\n",
      "  - 3\n",
      "  - 4\n",
      "  - 5\n",
      "  - 6\n",
      "  - 7\n",
      "  - 8\n",
      "9 - \n",
      "  - 10\n",
      "  - 11\n",
      "  - 12\n",
      "  - 13\n",
      "  - 14\n",
      "  - 15\n",
      "  - 16\n",
      "  - 17\n",
      "  constant_multiplier: 1.0\n",
      "  img_size: 256\n",
      "  mean:\n",
      "  - 623.2724609375\n",
      "  - 1247.657958984375\n",
      "  - 1772.24169921875\n",
      "  - 2371.256103515625\n",
      "2.867431640625\n",
      "  - 2357.759765625\n",
      "  no_data_value: -9999\n",
      "  reduce_to_zero: false\n",
      "  replace_label:\n",
      "- -9999\n",
      "  - -1\n",
      "  std:\n",
      "  - 2182.050048828125\n",
      "  - 2248.420654296875\n",
      "  - 2302.53515625\n",
      "03125372.2043457\n",
      "  - 2398.52685546875\n",
      "  - 2292.96435546875\n",
      "  temporal_dim: 3\n",
      "mean:\n",
      "- 670.5441284179688\n",
      "974853515625\n",
      "- 1772.599365234375\n",
      "- 2415.69091796875\n",
      "- 2879.2431640625\n",
      "- 2337.822509765625\n",
      "_inference\n",
      "model:\n",
      "  freeze_backbone: false\n",
      "  num_classes: 2\n",
      "output_dir: cwd/predictions\n",
      "root_dir: .\n",
      "std:\n",
      "- 2146.305419921875\n",
      "- 2203.416259765625\n",
      "- 2247.03515625\n",
      "- 2310.74755859375\n",
      "- 2322.708984375\n",
      "211.968505859375\n",
      "test:\n",
      "  crop_size: 256\n",
      "  img_size: 256\n",
      "  mask_cloud: false\n",
      "  stride: 256\n",
      "ath: test_ds.csv\n",
      "train:\n",
      "  batch_size: 16\n",
      "  class_weights:\n",
      "  - 1\n",
      "  - 1\n",
      "  ignore_index: -1\n",
      "rate: 0.0001\n",
      "  num_epochs: 20\n",
      "  weight_decay: 0.1\n",
      "train_filepath: null\n",
      "valid_filepath: null\n",
      "\n",
      "[2025-02-04 18:53:26,868][rasterio._env][INFO] - GDAL signalled an error: err_no=4, msg=\"`./.ipynb_checkpoints' not recognized as being in a supported file format.\"\n",
      "OR] - './.ipynb_checkpoints' not recognized as being in a supported file format.\n",
      "[2025-02-04 18:53:28,068][absl][INFO] - File '/home/krschap/.instageo/prithvi/Prithvi_EO_V1_100M.pt' already exists. Skipping download.\n",
      "tageo/prithvi/config.yaml' already exists. Skipping download.ins\n",
      "[2025-02-04 18:53:28,925][root][INFO] - GPU is available. Using GPU...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Running Inference: 100%|██████████| 151/151 [00:58<00:00,  2.57it/s]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python -m instageo.model.run --config-name=locust \\\n",
    "    root_dir=\".\" \\\n",
    "    test_filepath=\"test_ds.csv\" \\\n",
    "    train.batch_size=16 \\\n",
    "    checkpoint_path='cwd/instageo_best_checkpoint-v1.ckpt' \\\n",
    "    output_dir='cwd/predictions' \\\n",
    "    mode=chip_inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After getting the prdictions for each chip, we retrieve the predicted value for each observatio in our test split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "predictions_directory = \"cwd/predictions\"\n",
    "prediction_files = os.listdir(predictions_directory)\n",
    "\n",
    "def get_prediction_value(row):\n",
    "    matching_files = [f for f in prediction_files if (str(row['date']) in f) and (row['mgrs_tile_id'] in f)]\n",
    "    if not matching_files:\n",
    "        return (np.nan, np.nan)\n",
    "    for file in matching_files:\n",
    "        with rasterio.open(f\"{predictions_directory}/{file}\") as src:\n",
    "            width, height = src.width, src.height\n",
    "            affine_transform = rasterio.transform.AffineTransformer(src.transform)\n",
    "            transformer = Transformer.from_crs(CRS.from_epsg(4326), src.crs, always_xy=True)\n",
    "            x_chip, y_chip = transformer.transform(row['x'], row['y'])\n",
    "            x_offset, y_offset = affine_transform.rowcol(x_chip, y_chip)\n",
    "            \n",
    "            if 0 <= x_offset < width and 0 <= y_offset < height:\n",
    "                return src.read(1)[y_offset, x_offset], file\n",
    "    return (np.nan, np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in /home/krschap/foss/geoaihack2024/env/lib/python3.12/site-packages (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           id          x          y      date mgrs_tile_id\n",
      "0        ID_0  43.331667   7.276944  20210501        38NLP\n",
      "1        ID_1  44.902500  15.438056  20210601        38PMC\n",
      "2        ID_2  44.429167  16.330833  20210601        38QMD\n",
      "3        ID_3  65.844167  29.278611  20210201        41RQN\n",
      "4        ID_4  38.735556  17.148056  20210101        37QDU\n",
      "...       ...        ...        ...       ...          ...\n",
      "3213  ID_3213  45.377500   7.028333  20211001        38NNN\n",
      "3214  ID_3214  43.135833  15.619444  20210201        38PLC\n",
      "3215  ID_3215  57.328611  25.895278  20210101        40REP\n",
      "3216  ID_3216  62.626667  25.923611  20210101        41RMJ\n",
      "3217  ID_3217  47.930278  14.035556  20210301        38PRA\n",
      "\n",
      "[3218 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "submission_df = pd.read_csv(\"test.csv\")\n",
    "\n",
    "print(submission_df)\n",
    "\n",
    "submission_df[['prediction', 'filename']] = submission_df.apply(get_prediction_value, axis=1, result_type='expand')\n",
    "submission_df[[\"id\",\"prediction\"]].to_csv(\"hls_submission.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 10991118,
     "sourceId": 91683,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30839,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
